
<!-- saved from url=(0039)https://yzu-nlp.github.io// -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <title>自然语言处理(扬州大学）</title>
  <script src="./main.js"></script><script src="./jquery.min.js"></script><link rel="stylesheet" href="./main.css">
</head>

<body><div class="section">
  
  <div class="top assignmentTitle">
    自然语言处理(扬州大学）
  </div>

  <div class="top nav">
   2024年秋
  </div>
  <div class="top">
    <a class="nav" href="https://yzu-nlp.github.io/#info">[课程信息]</a>
    &nbsp;
    <a class="nav" href="https://yzu-nlp.github.io/#schedule">[课程计划]</a>
    &nbsp;
    <a class="nav" href="https://yzu-nlp.github.io/#coursework">[课程设计]</a>
    &nbsp;
    <a class="nav" href="https://yzu-nlp.github.io/#faq">[常问问题]</a>
 </div>
<br>



<div class="section">
  <a name="info"><span class="header">课程信息</span></a>
    <p class="subheader">任课教师:  <a href="https://qiang2100.github.io/">强继朋</a> </p>
       
  <p class="subheader">时间/地点:</p>
    <ul>
      <li>时间: 1-16周， 每周四上午1、2两节 8.00-9:40am
      </li>
      <li>地点: 扬子津西校区（07011524）
      </li>
   </ul>


  <p class="subheader">评分标准</p>
  <ul>
    <li><b>作业</b> (30%):
    将有四个作业，包括书面和编程部分。每个作业都以应用程序为中心，也将加深您对理论概念的理解。
    <b>注意:作业请独立完成。在扬大作业本完成</b>
    <ul>
        <li>作业 1: 语言模型, 朴素贝叶斯: </li>  
	    (Exercises 3.2-3.4, 3.6, 4.1-4.2)
        <li>作业 2: 文本分类、语义向量、神经网络 </li>  <a href="https://yzu-nlp.github.io/homeworks/homework2.docx">作业题目</a> 
        <li>作业 3: 循环神经网络、LSTM、 Transformer </li><a href="https://yzu-nlp.github.io/homeworks/homework3.docx">作业题目</a> 
        <li>作者 4: 预训练语言模型、大语言模型 </li><a href="https://yzu-nlp.github.io/homeworks/*">作业题目</a> 

    </ul>
    </li>

    <li><b>实验</b> (20%): 将有四次上机实验，通过动手编程，加深对概念的理解，实际使用NLP算法。
    <b>注意:实验报告请独立撰写完成。 </b>
     
    <ul>
        <li>实验 1: 构建N元语言模型 (5%)</li> <a href="https://yzu-nlp.github.io/exper/Experiment1.zip">实验1介绍</a> 
        <li>实验 2: 基于机器学习的情感分类 (5%)</li> <a href="https://yzu-nlp.github.io/exper/Experiment2-student.zip">实验2介绍</a> 
        <li>实验 3: 基于BERT的情感分类 (5%)</li>  <a href="https://yzu-nlp.github.io/exper/nlp2024/">实验3介绍</a>
	<li>实验 4: 基于大语言模型的情感分类 (5%)</li>  <a href="https://yzu-nlp.github.io/exper/nlp2024/">实验3介绍</a>
    </ul>
    实验报告模版 <a href="https://yzu-nlp.github.io/exper/report_template.docx">下载</a>
    </li>

    <li><b>期末考试</b> (50%): 考试周进行考试，闭卷考试，满分100分
   </li>

  </ul>

  <!-- <p class="subheader">Contact</p>
  Students should ask all course-related questions in the <a href="">Ed discussion</a> (not Piazza). For external enquiries, emergencies, or personal matters that you don't wish to put in a private Ed post, you can email us at cos484-584-staff@lists.cs.princeton.edu. -->

  <!-- </div> -->
  <!-- <p class="subheader">Useful Links:</p>
    <ul>
      <li>  <a href="">Canvas</a> </li>
      <li>  <a href="">Ed</a> for all questions related to lectures, homeworks, and projects, and to find announcements. For external queries, emergencies, or personal matters, you can use a private Piazza post visible only to Instructors.</li>
      <li>  Previous Offerings:  </li>
    </ul> -->
  <!--
  <div class="section"><b>Grades</b>: click <a href="restricted/grades.html">here</a> to check your grades and autograder feedback.</div>-->
  <p class="subheader">预备知识:</p>
    <ul>
    <li>需要: 概率论, 线性代数</li>
    <li> 熟练使用Python: 作业和实验需要使用 Python, Numpy 和 PyTorch.</li>
    <!-- <li> <a href="https://www.cs.princeton.edu/courses/archive/spring19/cos324/">COS 324</a> (or equivalent intro. to ML courses) is strongly recommended. </li> -->
    </ul>

  <p class="subheader">教材:</p>
    <ul>
      <li>Dan Jurafsky and James H. Martin. <a href="https://web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing (3rd ed. draft).</a></li>
      <li> 英文读起来困难，也建议慢慢阅读，长期受益。</li>
    </ul>

</div>


  <div class="section">
  <a name="schedule"><span class="header">课程计划</span></a>

  <p>课程安排是暂定的，可能会更改。所有作业均应在周四上午8点前提交。 </p>

  <table border="1" cellpadding="3" cellspacing="0" style="border-collapse: collapse" bordercolor="#111111" id="AutoNumber5" width="800">
    <tbody><tr>
      <td width="70" height="18"><b>Week</b></td>
      <td width="100" height="18"><b>Date</b></td>
      <td width="250" height="18"><b>Topics</b></td>
      <td width="350" height="18"><b>Readings</b></td>
      <td width="150" height="18"><b>Assignments</b></td>
    </tr>
    </tbody><tbody>
  <tr>
    <td>第1周</td>
    <td>8月29日</td>
    <td><a href="https://yzu-nlp.github.io/lectures/lec1-v2.pdf">NLP介绍</a></td>
    <td><a href="https://princeton-nlp.github.io/cos484/readings/advances_in_nlp.pdf">Advances in natural language processing</a></td>
    <td></td>
  </tr>
  <tr>
    <td> 第2周 </td>
    <td>9月5日</td>
    <td><a href="https://yzu-nlp.github.io/lectures/lec2-v2.pdf">语言模型</a></td>
    <td><a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf">J &amp; M 3.1-3.4</a></td>
    <td></td>
  </tr>
  <tr>
    <td> 第3周 </td>
    <td>9月12日</td>
    <td><a href="https://yzu-nlp.github.io/lectures/lec3.pdf">文本分类-朴素贝叶斯</a></td>
    <td><a href="https://web.stanford.edu/~jurafsky/slp3/4.pdf">J &amp; M 4.1-4.8</a></td>
    <td>作业1 </td>
  </tr>
	  
  <tr>
    <td> 第4周 </td>
    <td>9月19日</td>
    <td><a href="https://yzu-nlp.github.io/lectures/Logistic_Regression.pdf">文本分类-逻辑回归</a></td>
    <td><a href="https://web.stanford.edu/~jurafsky/slp3/4.pdf">J &amp; M 4.1-4.8</a></td>
    <td></td>
  </tr>

   
  <tr>
    <td> 第5周 <br> 第7周 </td>
    <td>9月26日 <br> 10月10日</td>
    <td><a href="https://yzu-nlp.github.io/lectures/lec4.pdf">向量语义:</a><br>
	1. 词的表示<br>
	2. 相似度度量<br>
	3. TF-IDF<br>
	4. Word2Vec
	  </td>
    <td>
      <!--- <a href="https://web.stanford.edu/~jurafsky/slp3/5.pdf">J & M 5.1-5.6</a> -->
      <a href="https://web.stanford.edu/~jurafsky/slp3/6.pdf">J &amp; M 6.2-6.4, 6.6</a>
      <br><a href="https://www.aclweb.org/anthology/P14-1023.pdf">Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors</a>
      <br><a href="https://arxiv.org/pdf/1301.3781.pdf">Efficient Estimation of Word Representations in Vector Space</a> (original word2vec paper)
      <br><a href="https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf">Distributed representations of words and phrases and their compositionality</a> (negative sampling)
      <!-- <br><a href="https://nlp.stanford.edu/pubs/glove.pdf">GloVe: Global Vectors for Word Representation</a>
      <br><a href="https://arxiv.org/pdf/1607.04606.pdf">Enriching Word Vectors with Subword Information</a> -->
    </td>
    <td> 作业2</td>
  </tr>

  <tr>
    <td> 第8周 <br> 第9周 </td>
    <td>10月17 <br> 10月24日 </td>
    <td><a href="https://yzu-nlp.github.io/lectures/lec5.pdf">神经语言模型: </a><br>
	1. 神经元<br>
	2. 前馈神经网络<br>
	3. 神经语言模型<br>
	4. 训练神经网络
    </td>
    <td>
    <a href="https://web.stanford.edu/~jurafsky/slp3/7.pdf">J & M 7.1-7.8</a>
      <br><a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A Neural Probabilistic Language Model</a>
      <br> <a href="https://princeton-nlp.github.io/cos484/readings/hmms-spring2013.pdf"> Notes from Michael Collins</a>
     <br>
     </td>
    
    <td> </td>
  </tr>
 
  <tr>
    <td> 第10周 <br> 第11周 </td>
    <td>10月31日  <br> 11月7日  </td>
    <td><a href="https://yzu-nlp.github.io/lectures/lec6.pdf">基于深度学习框架的序列处理:</a><br>
	1. RNN<br>
	2. LSTM<br>
	3. 编码器-解码器模型 <br>
	4. 注意力机制 
    </td>
    <td>
      <!--
      Notes from <a href="readings/em.pdf">Michael Collins</a> and (optional) <a href="readings/cs229-notes8.pdf">Andrew Ng</a>
      -->
      <a href="https://web.stanford.edu/~jurafsky/slp3/8.pdf">J&amp;M 8.1-8.8</a><br>
      <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a><br>
      <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a><br>
      <a href="http://proceedings.mlr.press/v37/jozefowicz15.pdf">An Empirical Exploration of Recurrent Network Architectures</a><br>
      <a href="https://arxiv.org/pdf/1603.01360.pdf">Neural Architectures for Named Entity Recognition</a> 
      <a href="https://arxiv.org/pdf/1409.0473.pdf">Neural Machine Translation by Jointly Learning to Align and Translate</a><br>
    </td>
    <td>作业3 </td>
  </tr>
 
   
  <tr>
    <td> 第12周 <br> 第13周 <br> 第14周 </td>
    <td>  11月14日 <br> 11月21日 <br> 11月28日</td>
    <td><a href="https://yzu-nlp.github.io/lectures/lec7.pdf">Transformer: </a> <br>
	1. 自注意力机制 <br>
	2. Transformer模块 <br>
	3. 预训练语言模型 <br>
	5. 预训练与微调 <br>
	6. ELMO、GPT、BERT <br>
       6. 机器翻译 <br>
    </td>
    <td><a href="https://web.stanford.edu/~jurafsky/slp3/9.pdf">J&amp;M 9.1-9.5</a><br>
    <a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a><br>
    <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a><br>
    <a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>
    </td>
    <td>作业四 </td>
  </tr>
 
  
  <tr>
    <td>第15周<br>第16周</td>
    <td>12月5日<br>12月12日</td>
    <td><a href="https://yzu-nlp.github.io/lectures/*">大语言模型:</a> <br>
	1. Post-BERT模型的预训练/微调 <br>
	2. GPT-3: 提示与上下文学习 <br>
	3. 指令微调、强化学习人类反馈（RLHF）、ChatGPT、GPT-4
	4. 大型语言模型的局限性

    </td>
    <td><a href="https://web.stanford.edu/~jurafsky/slp3/10.pdf">J&amp;M 10.1-10.6</a><br>
	    <a href="https://arxiv.org/pdf/1802.05365.pdf">Deep contextualized word representations</a><br>
    <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving Language Understanding by Generative Pre-Training</a><br>      
    <a href="https://arxiv.org/pdf/1810.04805.pdf">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a><br>
    <a href="https://arxiv.org/pdf/1907.10529.pdf">SpanBERT: Improving Pre-training by Representing and Predicting Spans</a><br>
     <a href="https://aclanthology.org/2021.emnlp-main.243.pdf">The Power of Scale for Parameter-Efficient Prompt Tuning</a>
     </td>
    <td></td>
  </tr>
 
</tbody>
  </table>

  </div>


<div class="section">
  <a name="coursework"><span class="header">课程设计</span></a>


  <p class="subheader">课题要求</p>
  <div class="section">
有两个选项：
（a）复现一篇ACL/NAACL/EMNLP 2021-2023年的论文（推荐选择）；
（b）完成一个研究项目（对于此选项，你需要先讨论你的提案并获得导师的事先批准）。

  </div>

  <div class="section"><b>分组介绍:</b>
    自行分组，每个小组最好3个成员，也可以2个成员（尽早找到你的队友！）。
  </div>

  <div class="section"><b>展示和提交的材料：</b>
    <ul>
      <li>PPT汇报</li>
      <li>每个同学需分别撰写自己的课程设计报告</li>
    </ul>
  </div>
  <div class="section"><b> 课程设计报告模版 <a href="https://yzu-nlp.github.io/exper/NLP课程设计-模版.docx">下载</a> </b>
  </div>
	
<br>
<div class="section">
  <a name="faq"><span class="header">常问问题</span></a>
  <ul>
    <li>
      <b>问题:  作业是收作业纸的还是作业本?</b> <br>回答:  作业是收纸质版的，需统一在扬大作业本上完成。
    </li><br>
    <li>
      <b>问题: XXXX?</b> <br> 回答: ******
    </li>
  </ul>
</div>


<div class="section">
<span class="header">  参考课程：</span>  <a href="https://princeton-nlp.github.io/cos484/">普林斯顿NLP课程</a>

</div>


</body></html>
